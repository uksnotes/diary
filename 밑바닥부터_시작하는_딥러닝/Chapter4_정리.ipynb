{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4_정리.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i25x46_WTpr_"
      },
      "source": [
        "## **4.1** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BgrI6mYN8k"
      },
      "source": [
        "- 학습 : 훈련 데이터로 가중치의 최적값을 획득 \n",
        "- 기계 학습 : 수집한 데이터로부터 패턴을 찾음\n",
        "- 이미지 분류 : 이미지를 백터로 변환하고 SVM KNN으로 학습\n",
        "- 사람이 생각하는 알고리즘 -> 결과\n",
        "- 사람이 생각하는 특징 -> 기계 학습(SVM, KNN) -> 결과\n",
        "- 딥러닝 -> 결과\n",
        "- 딥러닝 : 사람의 개입이 전혀없는 end to end machine learning 이라고 부름\n",
        "- 데이터 입력에서 목표한 결과까지 사람의 개입없이 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zgl5txxV-oc"
      },
      "source": [
        "## **4.2** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qW0tLi7WDHh"
      },
      "source": [
        "- 사람은 행복하다로 표현하지만 기계는 가중치 매개변수 값으로 표현\n",
        "- 신경망에서 사용하는 지표는 손실 함수는 대개 오차제곱합으로 계산\n",
        "- 오차제곱합 = (1/2)*sum(y_k-t_k)^2\n",
        "- t = [0, 0, 1, 0, 0] y = [0.1, 0.2, 0.6, 0.1, 0]\n",
        "- 답이 2일 때 그리고 확률이 가장 높을 때 오차제곱합이 가장 작음 \n",
        "- 예를 들어 y = [0.1, 0.2, 0.1, 0.6, 0]일 때 답을 3으로 예측할 때 오차제곱합과 비교해서 첫번째 예시가 오차제곱합이 훨씬 작음\n",
        "- 교차 엔트로피 오차 : E = -sum(t_k*logy_k) \n",
        "- 정답이 2이고 신경망 출력이 0.6일 때 -log0.6 ~ 0.51이 되고 신경망 출력이 0.1일 때 -log0.1 ~ 2.3이 되서 0.6일 때 더 작게 나옴\n",
        "- 미니배치 학습 : 예를 들어 무작위로 100장을 뽑아서 학습 \n",
        "- 신경망에서 정확도를 지표로 삼으면 안됨: 매개변수의 미분이 대부분의 장소에서 0이 됨 (시그모이드 함수를 이용하는 이유)\n",
        "- 예를 들어 100장에서 32를 맞추면 32% 하지만 32.13215라는 연속적인 숫자로 표현이 안되는 현상 발생 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kupLBCblWC9f"
      },
      "source": [
        "## **4.3** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmlB_GVqWC02"
      },
      "source": [
        "- df(x)/dx = lim h->0 f(x+h)-f(x)/h로 미분을 구할 수 있음\n",
        "- 소수점 8번째 자리를 넘어가게 되면 반올림하는 오류가 발생 \n",
        "- 미세한 차이지만 모델에서는 큰 차이를 보임\n",
        "- 중앙 차분으로 오류 해결 : df(x)/dx = lim h->0 f(x+h)-f(x-h)/h\n",
        "- 편미분 x_0^2 + x_1^2 이 존재할 때 df/dx_0를 하면 2x_0으로 결과값 출력 \n",
        "- 편미분은 특정 장소의 기울기를 구함 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9-DDr5Ag-P"
      },
      "source": [
        "## **4.4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-K6gowAkGW"
      },
      "source": [
        "- (af/ax_0, af/ax_1) 기울기로 정의\n",
        "- f(x) = x_0^2 + x_1^2\n",
        "- 편미분을 하였을 때 나침반처럼 화살표들은 한 점을 향함 \n",
        "- 가장 낮은 곳에서 멀어질수록 화살표 크기가 커짐 \n",
        "- 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향 \n",
        "- 신경망 역시 최적의 매개변수(가중치와 편향)을 학습 시에 찾음\n",
        "- 기울기를 잘 이용해 함수의 최솟값을 찾는 과정을 경사법이라고 칭함 \n",
        "- 안정점 기울기가 0이 되어 함수가 최솟 값을 출력 \n",
        "- 경사법 : 현 위치에서 기울어진 방향으로 일정 거리 이동한 후 다음 곳에서 기울기를 구해서 이동\n",
        "- x_0 = x_0 - learning_rate*af/ax_0\n",
        "- x_1 = x_1 - learning_rate*af/ax_1\n",
        "- learning rate : 한 번에 얼마만큼 학습할 지 정함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JgbFCMADZqt"
      },
      "source": [
        "import numpy as np\n",
        "def numeric_gradient(f, x):\n",
        "    h = 1e-4\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        temp_val = x[idx]\n",
        "        x[idx] = temp_val + h\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        x[idx] = temp_val - h\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = temp_val\n",
        "\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbURQP8DIDR"
      },
      "source": [
        "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
        "    x = init_x\n",
        "\n",
        "    for i in range(step_num):\n",
        "        grad = numeric_gradient(f, x)\n",
        "        x -= lr*grad\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLZH0IHDNgh",
        "outputId": "22ed4554-bc1d-4f41-9153-8793771f7959"
      },
      "source": [
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11110793e-10,  8.14814391e-10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYlRY4_rESMI"
      },
      "source": [
        "- 거의 (0, 0)에 가까운 결과를 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwHhSc16El9W",
        "outputId": "448e25a6-4533-4071-fd75-a49cd4a394a4"
      },
      "source": [
        "gradient_descent(function_2, init_x = init_x, lr = 10, step_num = 100) # learning rate가 클 때 큰 수를 반환 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.18122051e+12, -3.96419374e+12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWG2occEwKP",
        "outputId": "de77e0f5-70f6-4746-fed4-dcdbce6a4d04"
      },
      "source": [
        "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100) # learning rate가 작을 때 작은 수를 반환"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.18122051e+12, -3.96419374e+12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1EiO9HrE1Ti"
      },
      "source": [
        "- 하이퍼파라매터 : learn_rate을 같이 사람이 직접 설정해야 하는 매개 변수 값 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1KiHCcHfii"
      },
      "source": [
        "## **4.5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSi4tE4MHjsA"
      },
      "source": [
        "- 신경망 : 적용 가능한 가중치와 편향이 있고 훈련 데이터에 적응 및 조정 과정을 학습이라 칭함 \n",
        "- 1단계(미니배치) : 미니배치 훈련 데이터 중 일부를 무작위로 가져옴\n",
        "- 2단계(기울기 산출) : 가중치 매개변수의 기울기를 구함 (손실 함수의 값을 가장 작게 하는 방향)\n",
        "- 3단계(매개변수 갱신) : 가중치 매개변수를 기울기 방향으로 갱신 \n",
        "- 4단계(반복) : 1단계에서 3단계를 반복 \n",
        "- 데이터를 미니 배치로 무작위로 선정해 확률적 경사 하강법(SGD)라고 칭함 \n",
        "- TwoLayerNet() 임의로 만든 함수는 __init__(self, input_size, hidden_size, output_size) 클래스를 초기화 \n",
        "- 손글씨 숫자 인식에서 28*28 입력 이미지가 있고 출력은 0-9로 10개가 존재 \n",
        "- Input_size는 784, output_size는 10개가 됨을 확인 \n",
        "- Hidden_size는 적당한 값을 지정 \n",
        "- 가중치 감소 드롭아웃을 통해 과적합을 막을 수 있음 \n",
        "- epochs을 사용해 모델을 여러 번 학습 시킴 \n",
        "- 과적합은 train_dataset과 test_dataset이 겹쳐 있으면 발생하지 않으나 실제 데이터와 겹쳐져 있지 않을 때 과적합이 발생  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7EeOvP3OQb"
      },
      "source": [
        "params -> params['w1'] params['b1'] 가중치와 편향을 나타냄 \n",
        "params['w2'] params['b2'] 2번째 층의 가중치와 편향을 나타냄\n",
        "grads['w1'] grad['b1'] 1번째 가중치의 기울기와 편향을 나타냄\n",
        "grads['w2'] grad['b2'] 2번째 가중치의 기울기와 편향을 나타냄\n",
        "\n",
        "for key in ('w1', 'b1', 'w2', 'b2'):\n",
        "    params[key] -= learning_rate * grads[key]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}